{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_WverkHQl14"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real_data = pd.read_csv(\"output.csv\")"
      ],
      "metadata": {
        "id": "4PtHKi_khlBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_data. drop(['n','MRI'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "frKMV3DY_QJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input layer (conditioning variables and noise)\n",
        "    model.add(Dense(units=64, input_shape=(noise_dim,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    # Additional fully connected layers with BatchNormalization and LeakyReLU activation\n",
        "    model.add(Dense(units=64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    # Output layer with 6 units (matches the desired output shape)\n",
        "    model.add(Dense(units=6, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Discriminator network\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input layer (real or generated data)\n",
        "    model.add(Dense(units=64, input_shape=(6,)))  # Input shape matches the data shape\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    # Additional fully connected layers with BatchNormalization and LeakyReLU activation\n",
        "    model.add(Dense(units=64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    # Output layer (binary classification)\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "DUonQYURQy03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_crossentropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)"
      ],
      "metadata": {
        "id": "yqPZiRHSM3s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = binary_crossentropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "# Generator loss function\n",
        "def generator_loss(fake_output):\n",
        "    return binary_crossentropy(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "Nrw_TfEeRiN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, real_data, num_steps, batch_size, noise_dim, print_interval):\n",
        "    generated_data_history = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        noise = tf.random.normal([batch_size, noise_dim])\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_data = generator(noise)\n",
        "            real_output = discriminator(real_data)\n",
        "            fake_output = discriminator(generated_data)\n",
        "            gen_loss = generator_loss(fake_output)\n",
        "            disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "        generated_data_history.append(generated_data.numpy())  # Convert to numpy and append\n",
        "\n",
        "        if (step + 1) % print_interval == 0:\n",
        "            print(f\"Step {step + 1}: Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}\")\n",
        "\n",
        "    # Reshape the final generated data to (4806, 6)\n",
        "    generated_data_history = np.concatenate(generated_data_history, axis=0)\n",
        "    generated_data_history = generated_data_history[:num_steps, :]  # Ensure the correct shape\n",
        "\n",
        "    return generated_data_history\n"
      ],
      "metadata": {
        "id": "phxWnBM8RifO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "num_steps = 4806  # Number of training steps (adjust as needed)\n",
        "batch_size = 6  # Batch size for each training step (adjust as needed)\n",
        "noise_dim = 6  # Dimension of random noise input to the generator (adjust as needed)\n",
        "print_interval = 100  # Print loss values every 'print_interval' steps (adjust as needed)\n",
        "\n",
        "# Assuming your real_data is a pandas DataFrame, you can convert it to a NumPy array like this:\n",
        "real_data_np = real_data.values\n",
        "\n",
        "# Now, call the train_gan function with the NumPy array\n",
        "generated_data_history = train_gan(generator, discriminator, real_data_np, num_steps, batch_size, noise_dim, print_interval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn1MeW3RRig1",
        "outputId": "761a173b-442c-400e-f191-80dbd5699725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100: Generator Loss: 0.8822280764579773, Discriminator Loss: 0.5343238711357117\n",
            "Step 200: Generator Loss: 1.1084975004196167, Discriminator Loss: 0.4006047248840332\n",
            "Step 300: Generator Loss: 1.3687912225723267, Discriminator Loss: 0.29358628392219543\n",
            "Step 400: Generator Loss: 1.5969997644424438, Discriminator Loss: 0.22632302343845367\n",
            "Step 500: Generator Loss: 1.8214393854141235, Discriminator Loss: 0.17649133503437042\n",
            "Step 600: Generator Loss: 2.0607283115386963, Discriminator Loss: 0.13623358309268951\n",
            "Step 700: Generator Loss: 2.276641607284546, Discriminator Loss: 0.10828515142202377\n",
            "Step 800: Generator Loss: 2.4781246185302734, Discriminator Loss: 0.08763021230697632\n",
            "Step 900: Generator Loss: 2.662384033203125, Discriminator Loss: 0.07233595103025436\n",
            "Step 1000: Generator Loss: 2.8351802825927734, Discriminator Loss: 0.060501858592033386\n",
            "Step 1100: Generator Loss: 2.8819000720977783, Discriminator Loss: 0.05765913426876068\n",
            "Step 1200: Generator Loss: 3.039487600326538, Discriminator Loss: 0.049042582511901855\n",
            "Step 1300: Generator Loss: 3.181452512741089, Discriminator Loss: 0.04241211339831352\n",
            "Step 1400: Generator Loss: 3.31134033203125, Discriminator Loss: 0.037148814648389816\n",
            "Step 1500: Generator Loss: 3.433633804321289, Discriminator Loss: 0.032801613211631775\n",
            "Step 1600: Generator Loss: 3.567344903945923, Discriminator Loss: 0.028636856004595757\n",
            "Step 1700: Generator Loss: 3.7005882263183594, Discriminator Loss: 0.02501937933266163\n",
            "Step 1800: Generator Loss: 3.8247668743133545, Discriminator Loss: 0.022065185010433197\n",
            "Step 1900: Generator Loss: 3.941645860671997, Discriminator Loss: 0.019607199355959892\n",
            "Step 2000: Generator Loss: 4.0535359382629395, Discriminator Loss: 0.01751333847641945\n",
            "Step 2100: Generator Loss: 4.163204193115234, Discriminator Loss: 0.015679920092225075\n",
            "Step 2200: Generator Loss: 4.267848014831543, Discriminator Loss: 0.01411100197583437\n",
            "Step 2300: Generator Loss: 4.368032455444336, Discriminator Loss: 0.01275718491524458\n",
            "Step 2400: Generator Loss: 4.464274883270264, Discriminator Loss: 0.011579826474189758\n",
            "Step 2500: Generator Loss: 4.557276248931885, Discriminator Loss: 0.010546013712882996\n",
            "Step 2600: Generator Loss: 4.65009880065918, Discriminator Loss: 0.00960665475577116\n",
            "Step 2700: Generator Loss: 4.740098476409912, Discriminator Loss: 0.008776181377470493\n",
            "Step 2800: Generator Loss: 4.827365398406982, Discriminator Loss: 0.008039822801947594\n",
            "Step 2900: Generator Loss: 4.912453651428223, Discriminator Loss: 0.007381597068160772\n",
            "Step 3000: Generator Loss: 4.995555400848389, Discriminator Loss: 0.006790968123823404\n",
            "Step 3100: Generator Loss: 5.076652526855469, Discriminator Loss: 0.00626031868159771\n",
            "Step 3200: Generator Loss: 5.155925273895264, Discriminator Loss: 0.00578182702884078\n",
            "Step 3300: Generator Loss: 5.233343124389648, Discriminator Loss: 0.005349944811314344\n",
            "Step 3400: Generator Loss: 5.3085618019104, Discriminator Loss: 0.004961326718330383\n",
            "Step 3500: Generator Loss: 5.382026195526123, Discriminator Loss: 0.004609100054949522\n",
            "Step 3600: Generator Loss: 5.454253673553467, Discriminator Loss: 0.004287246614694595\n",
            "Step 3700: Generator Loss: 5.5248284339904785, Discriminator Loss: 0.003994520287960768\n",
            "Step 3800: Generator Loss: 5.594466686248779, Discriminator Loss: 0.003725313348695636\n",
            "Step 3900: Generator Loss: 5.663247585296631, Discriminator Loss: 0.0034772641956806183\n",
            "Step 4000: Generator Loss: 5.731233596801758, Discriminator Loss: 0.0032483432441949844\n",
            "Step 4100: Generator Loss: 5.798480987548828, Discriminator Loss: 0.00303676538169384\n",
            "Step 4200: Generator Loss: 5.865042209625244, Discriminator Loss: 0.0028409340884536505\n",
            "Step 4300: Generator Loss: 5.930959701538086, Discriminator Loss: 0.00265946495346725\n",
            "Step 4400: Generator Loss: 5.996288299560547, Discriminator Loss: 0.002491069957613945\n",
            "Step 4500: Generator Loss: 6.061062335968018, Discriminator Loss: 0.0023346447851508856\n",
            "Step 4600: Generator Loss: 6.125319957733154, Discriminator Loss: 0.0021891866344958544\n",
            "Step 4700: Generator Loss: 6.189085483551025, Discriminator Loss: 0.0020538095850497484\n",
            "Step 4800: Generator Loss: 6.252394199371338, Discriminator Loss: 0.0019276943057775497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generated_data_history = np.array(generated_data_history)"
      ],
      "metadata": {
        "id": "A4IWfu7XJHJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(real_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "090nvzJOOLB6",
        "outputId": "159e6c04-d754-4ec2-f6b1-7882ab05eea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4806, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data_history.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVtB5LxVPriO",
        "outputId": "4155969f-20ff-48cb-fdc6-dbc4cec05917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4806, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generated_data_history = np.concatenate(generated_data_history, axis=0)\n",
        "# generated_data_history = generated_data_history.reshape((num_steps, 6))"
      ],
      "metadata": {
        "id": "NU25VX06HujI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9PYjusv4HzYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_data_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z9_ESXR_0NA",
        "outputId": "46c284ea-08c8-4955-f274-9b09bea53653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6.1409974e-01 4.8928002e-01 4.4327116e-01 4.6422887e-01 5.0104189e-01\n",
            "  4.3839070e-01]\n",
            " [5.1705325e-01 4.8766190e-01 4.1717219e-01 5.0290108e-01 5.6601638e-01\n",
            "  4.7729322e-01]\n",
            " [5.5335402e-01 5.1085925e-01 4.1242447e-01 4.1745034e-01 4.7230238e-01\n",
            "  4.8911899e-01]\n",
            " ...\n",
            " [9.9912697e-01 5.9518492e-04 9.9387497e-01 1.2914952e-03 9.9601597e-01\n",
            "  5.1720394e-04]\n",
            " [9.9907255e-01 6.8194408e-04 9.9605620e-01 4.8407535e-03 9.9478650e-01\n",
            "  4.5299222e-04]\n",
            " [9.9969649e-01 2.4617769e-04 9.9774677e-01 8.8149653e-04 9.9727422e-01\n",
            "  8.8669294e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def calculate_performance_metrics(real_data_np, generated_data_history):\n",
        "    rmse = np.sqrt(mean_squared_error(real_data_np, generated_data_history))\n",
        "    correlation = np.corrcoef(real_data_np, generated_data_history)[0, 1]\n",
        "    mae = mean_absolute_error(real_data_np, generated_data_history)\n",
        "    return rmse, correlation, mae\n"
      ],
      "metadata": {
        "id": "8lUMIJxVRik4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse, correlation, mae = calculate_performance_metrics(real_data_np, generated_data_history)\n",
        "\n",
        "# Print the computed metrics\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"Correlation: {correlation}\")\n",
        "print(f\"MAE: {mae}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33TJi4_jCXPi",
        "outputId": "b63587a8-5d2f-4c79-fed6-3133148f5a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 4529.259276470093\n",
            "Correlation: 0.9936722988189369\n",
            "MAE: 1739.1569243684762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfOCrrlpFjrV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}